{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_level.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "napuvyAruk75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import re\n",
        "import itertools\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, TimeDistributed, Activation\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEEcK0tEvGsH",
        "colab_type": "code",
        "outputId": "752e9a53-fa2e-43a9-ac83-acc12666e5d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "wiki103_folder = os.path.join(os.getcwd(), 'wikitext-103')\n",
        "\n",
        "wiki103_paths = [os.path.join(wiki103_folder, file_name) for file_name in os.listdir(wiki103_folder)]\n",
        "wiki103_paths"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Cinnamon/a7/wikitext-103/wiki.train.tokens',\n",
              " '/content/drive/My Drive/Cinnamon/a7/wikitext-103/wiki.test.tokens',\n",
              " '/content/drive/My Drive/Cinnamon/a7/wikitext-103/wiki.valid.tokens']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI53SYlvvI7b",
        "colab_type": "code",
        "outputId": "f256d98e-8d0f-435f-94b6-12f8f544c02d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "wiki2_folder = os.path.join(os.getcwd(), 'wikitext-2')\n",
        "\n",
        "wiki2_paths = [os.path.join(wiki2_folder, file_name) for file_name in os.listdir(wiki2_folder)]\n",
        "wiki2_paths"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Cinnamon/a7/wikitext-2/wiki.train.tokens',\n",
              " '/content/drive/My Drive/Cinnamon/a7/wikitext-2/wiki.valid.tokens',\n",
              " '/content/drive/My Drive/Cinnamon/a7/wikitext-2/wiki.test.tokens']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPmXQdPnvMuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(folder_path):\n",
        "    \n",
        "    def get_text(file_name):\n",
        "        \n",
        "        path = os.path.join(folder_path, file_name)\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            document = []\n",
        "            table = str.maketrans('', '', string.punctuation)\n",
        "            for sentence in tqdm(f, desc=f'PROCESSING {file_name}', position=0):\n",
        "                tokens = sentence.lower().split()\n",
        "                tokens = [w.translate(table) for w in tokens]\n",
        "                tokens = [word for word in tokens if word.isalpha()]\n",
        "                document.append(tokens)\n",
        "        return list(itertools.chain.from_iterable(document))\n",
        "\n",
        "    train_text = get_text('wiki.train.tokens') \n",
        "    valid_text = get_text('wiki.valid.tokens') \n",
        "    test_text = get_text('wiki.test.tokens')\n",
        "\n",
        "    return [train_text, \n",
        "            valid_text, \n",
        "            test_text] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud2022ZQvRDr",
        "colab_type": "code",
        "outputId": "492cc5e0-1183-4570-9309-5b4152d117a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "train_text, valid_text, test_text = get_data(wiki2_folder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PROCESSING wiki.train.tokens: 36718it [00:01, 24397.50it/s]\n",
            "PROCESSING wiki.valid.tokens: 3760it [00:00, 19010.62it/s]\n",
            "PROCESSING wiki.test.tokens: 4358it [00:00, 20806.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY1FBThzkYk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate word frequency and discard less frequent words\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M949SqILmtTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQUENCE_LEN = 10               # generate n-grams\n",
        "train_sentences = []\n",
        "train_next_words = []\n",
        "for i in range(0, len(train_text) - SEQUENCE_LEN):\n",
        "    # Only add sequences where no word is in ignored_words\n",
        "    if len(set(train_text[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
        "        train_sentences.append(train_text[i: i + SEQUENCE_LEN])\n",
        "        train_next_words.append(train_text[i + SEQUENCE_LEN])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZag0F6gozJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_sentences = []\n",
        "valid_next_words = []\n",
        "for i in range(0, len(valid_text) - SEQUENCE_LEN):\n",
        "    if len(set(valid_text[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
        "        valid_sentences.append(valid_text[i: i + SEQUENCE_LEN])\n",
        "        valid_next_words.append(valid_text[i + SEQUENCE_LEN])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBgPHX2YopkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sentences = []\n",
        "test_next_words = []\n",
        "for i in range(0, len(test_text) - SEQUENCE_LEN):\n",
        "    if len(set(test_text[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
        "        test_sentences.append(test_text[i: i + SEQUENCE_LEN])\n",
        "        test_next_words.append(test_text[i + SEQUENCE_LEN])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byUR6ysCpgYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_data(sentence_list, next_word_list, batch_size):\n",
        "    index = 0\n",
        "    while True:\n",
        "        x = np.zeros((batch_size, SEQUENCE_LEN, len(words)), dtype=np.bool)\n",
        "        y = np.zeros((batch_size, len(words)), dtype=np.bool)\n",
        "        for i in range(batch_size):\n",
        "            for t, w in enumerate(sentence_list[index]):\n",
        "                x[i, t, word_indices[w]] = 1\n",
        "            y[i, word_indices[next_word_list[index]]] = 1\n",
        "\n",
        "            index = index + 1\n",
        "            if index == len(sentence_list):\n",
        "                index = 0\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb9I46JFpw2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_generator = generate_data(train_sentences, train_next_words, batch_size=BATCH_SIZE)\n",
        "validation_generator = generate_data(valid_sentences, valid_next_words, batch_size=BATCH_SIZE)\n",
        "test_generator = generate_data(test_sentences, test_next_words, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tfyT5TgSnged",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(SEQUENCE_LEN, len(words))))\n",
        "model.add(Dense(len(words)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train the model and save to the Model folder\n",
        "checkpointer = ModelCheckpoint(filepath=os.path.join('./model/{val_acc:.2f}.h5'), save_best_only=True, verbose=1)\n",
        "model.fit_generator(train_generator, steps_per_epoch=len(train_sentences)//BATCH_SIZE, epochs=10,\n",
        "                    validation_data=validation_generator, validation_steps=len(valid_sentences)//BATCH_SIZE,\n",
        "                    callbacks=[checkpointer])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xatfXroFb831",
        "colab_type": "text"
      },
      "source": [
        "## These code adapted from my previous [project](https://github.com/KingLeo2000/Vietnamese-Spell-Correction/blob/master/train_spell.ipynb) at university "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6qU40kSYMLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds)\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4kZVMca3Jqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(seed, max_word=30):\n",
        "    sentence = seed.split()\n",
        "    for i in range(max_word):\n",
        "        x_pred = np.zeros((1, SEQUENCE_LEN, len(words)))\n",
        "        for t, word in enumerate(sentence):\n",
        "            x_pred[0, t, word_indices[word]] = 1.\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_word = indices_word[next_index]\n",
        "\n",
        "        sentence = sentence[1:]\n",
        "        sentence.append(next_word)\n",
        "\n",
        "        print(' '+next_word, end='')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqEajhoqSkvH",
        "colab_type": "code",
        "outputId": "5aa65cfb-f2b2-4dac-edb1-6de00eca8a2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed = ' '.join(test_sentences[1])\n",
        "seed"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'unk robert unk is an english film television and theatre'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux5ywUlbX0ix",
        "colab_type": "code",
        "outputId": "48c88cad-d3b3-4e70-b1a8-4f79f3f386c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "generate_text(seed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " it stands over the german reviews including seven croatian six from hull and november the palace and jews any club for years with mark of service and participated in the"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}